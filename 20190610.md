> 本次周报尝试使用**markdown**语法完成，以增强可读性。

# 工时：40h
* 毕设相关：30%
* 敲代码：60%
* 听讲座：10%
---

# 上周计划
1. 考虑在华为项目当前代码中加入生成方法；
2. 理解机器人的控制代码。

# 上周概要
1. 听了 Ryan Farrell 博士关于细粒度识别的讲座；
2. 学习了 Android 编程中 Activity、Service、Broadcast Receiver 和 Intent 的概念（用于理解机器人的控制代码）；
3. 学习并尝试使用 TensorFlow 中的动态图 Eager Execution 改写代码（注1）。
---

# Work Summary
1. Have a lecture on fine-grained recognition by Dr. Ryan Farrell.
2. Learn about the concept of Activity, Service, Broadcast Receiver and Intent in Android programming in order to understand the control codes of the robot.
3. Learn and using eager execution in TensorFlow to rewrite the codes.
---

# 附注

## 注1：TensorFlow 中的 Eager Execution

### 什么是 Eager Execution
> TensorFlow 的 Eager Execution 是一种命令式编程环境，可立即评估操作，无需构建图：操作会返回具体的值，而不是构建以后再运行的计算图。

简而言之，之前使用 TensorFlow 需要先按规范构建数据集，并将计算图（即网络）静态描述出来（此时不提供数据），然后以数据流的形式使数据通过图，完成前向/反向传播和参数更新，称为**静态图**框架。而启用了 Eager Execution 的 TensorFlow 与 pytorch 相似，省去了构建数据集的步骤，在描述计算图时直接进行计算，称为**动态图**框架。

### 为什么要用Eager Execution
> There are multiple changes in TensorFlow 2.0 to make TensorFlow users more productive. TensorFlow 2.0 removes redundant APIs, makes APIs more consistent (Unified RNNs, Unified Optimizers), and better integrates with the Python runtime with **Eager execution**.

上周 TensorFlow 发布了 2.0 Beta 版本，Eager Execution 成为 2.0 版本的主要特性之一。事实上 TensorFlow 从 1.5 版本就支持了 Eager Execution（最新的发行版是 1.14），主要解决的就是静态图**不够灵活**的问题。顾名思义，静态图**在数据集和网络结构固定**时有很好的表现，提前构建数据集和计算图可以更高效地利用资源；而我们的研究（特别是增量学习）在训练中**需要多次调整数据集内容甚至网络结构**，使用静态图需要在每次调整时重启整个训练过程，带来了极大的不便（甚至无法实现），这一点在懿荣师兄的毕业分享中也提到过：

> 由于网络结构是变化的，使用 TensorFlow 很难复现，最后使用 pytorch 复现成功。

同时在和丰远、俊廷的讨论时，他们也提到在目前的代码框架（即 hw_v2）中只能通过**很不优雅**的方式实现小样本模式的数据读入：即需要对数据的读入顺序进行深入定制时（例如需要连续5个样本来自同一类别），只能通过在构建数据集时将读入顺序以列表的方式固化下来；需要随机噪声作为输入时，也需要提前准备一个噪声池，从中顺序地抽取噪声。这样会使数据集利用不够充分，**使数据的多样性下降**。在接下来使用 Eager Execution 的版本（即 hw_v3）中，我会解决这个痛点：由于动态图的特性，数据读入将会和 pytorch 没有区别，顺序可以按照 python 的语法任意定制。

除此上述提到的好处之外，由于不需要提前构建计算图，启用 Eager Execution 后的代码也会更简洁。一般认为动态图的缺点是相比静态图运行效率会降低，但根据官方代码的测试，在我们使用的 resnet_v1_50 网络上，动态图和静态图的运行速度**没有显著差距**：

Benchmark name | batch size | images/second
:-:|:-:|:-:
eager_train_gpu_batch_32_channels_first | 32 | 171
graph_train_gpu_batch_32_channels_first | 32 | 172

### 当前进度

目前基础网络的训练代码已经基本完成，正在准备调试运行。在编写代码的过程中确实感受到上述动态图的优势，而实际运行效率是否有下降还有待测试。
